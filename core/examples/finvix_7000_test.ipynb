{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finvix 7000 Test - Model Response Evaluation\n",
    "\n",
    "This notebook processes the finvix_7000.json file and only performs evaluation/matching when the `model_response` key is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List, Optional\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the finvix_7000.json file\n",
    "def load_finvix_data(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load the finvix JSON data from file\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Load data\n",
    "finvix_data = load_finvix_data('finvix_7000.json')\n",
    "print(f\"Total records loaded: {len(finvix_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter records that have model_response key\n",
    "def filter_records_with_model_response(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Filter records that contain the 'model_response' key\n",
    "    \"\"\"\n",
    "    filtered_data = []\n",
    "    for record in data:\n",
    "        if 'model_response' in record and record['model_response'] is not None:\n",
    "            filtered_data.append(record)\n",
    "    return filtered_data\n",
    "\n",
    "# Filter data\n",
    "filtered_data = filter_records_with_model_response(finvix_data)\n",
    "print(f\"Records with model_response: {len(filtered_data)}\")\n",
    "print(f\"Records without model_response: {len(finvix_data) - len(filtered_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse ground truth JSON strings\n",
    "def parse_ground_truth(ground_truth_str: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse the ground truth JSON string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return json.loads(ground_truth_str)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "# Process filtered data for evaluation\n",
    "evaluation_data = []\n",
    "for record in filtered_data:\n",
    "    ground_truth = parse_ground_truth(record.get('final_ground_truth', '{}'))\n",
    "    model_response = record.get('model_response', {})\n",
    "    \n",
    "    evaluation_record = {\n",
    "        'ground_truth': ground_truth,\n",
    "        'model_response': model_response,\n",
    "        'instructions': record.get('final_instructions', ''),\n",
    "        'input': record.get('final_input', '')\n",
    "    }\n",
    "    evaluation_data.append(evaluation_record)\n",
    "\n",
    "print(f\"Prepared {len(evaluation_data)} records for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field-level accuracy calculation\n",
    "def calculate_field_accuracy(evaluation_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Calculate field-level accuracy between ground truth and model response\n",
    "    \"\"\"\n",
    "    field_stats = {}\n",
    "    \n",
    "    for record in evaluation_data:\n",
    "        ground_truth = record['ground_truth']\n",
    "        model_response = record['model_response']\n",
    "        \n",
    "        # Get all unique fields from both ground truth and model response\n",
    "        all_fields = set(ground_truth.keys()) | set(model_response.keys())\n",
    "        \n",
    "        for field in all_fields:\n",
    "            if field not in field_stats:\n",
    "                field_stats[field] = {\n",
    "                    'total': 0,\n",
    "                    'correct': 0,\n",
    "                    'ground_truth_present': 0,\n",
    "                    'model_response_present': 0\n",
    "                }\n",
    "            \n",
    "            field_stats[field]['total'] += 1\n",
    "            \n",
    "            # Check if field is present in ground truth\n",
    "            if field in ground_truth:\n",
    "                field_stats[field]['ground_truth_present'] += 1\n",
    "            \n",
    "            # Check if field is present in model response\n",
    "            if field in model_response:\n",
    "                field_stats[field]['model_response_present'] += 1\n",
    "            \n",
    "            # Check if values match (only if both are present)\n",
    "            if field in ground_truth and field in model_response:\n",
    "                gt_value = str(ground_truth[field]).strip().lower() if ground_truth[field] is not None else ''\n",
    "                mr_value = str(model_response[field]).strip().lower() if model_response[field] is not None else ''\n",
    "                \n",
    "                if gt_value == mr_value:\n",
    "                    field_stats[field]['correct'] += 1\n",
    "    \n",
    "    # Calculate accuracy percentages\n",
    "    field_accuracy = {}\n",
    "    for field, stats in field_stats.items():\n",
    "        total_comparisons = min(stats['ground_truth_present'], stats['model_response_present'])\n",
    "        accuracy = (stats['correct'] / total_comparisons * 100) if total_comparisons > 0 else 0\n",
    "        \n",
    "        field_accuracy[field] = {\n",
    "            'accuracy': accuracy,\n",
    "            'correct': stats['correct'],\n",
    "            'total_comparisons': total_comparisons,\n",
    "            'ground_truth_present': stats['ground_truth_present'],\n",
    "            'model_response_present': stats['model_response_present']\n",
    "        }\n",
    "    \n",
    "    return field_accuracy\n",
    "\n",
    "# Calculate field accuracy\n",
    "field_accuracy = calculate_field_accuracy(evaluation_data)\n",
    "print(\"Field-level accuracy calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display field accuracy results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIELD-LEVEL ACCURACY RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "accuracy_df = pd.DataFrame.from_dict(field_accuracy, orient='index')\n",
    "accuracy_df = accuracy_df.sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(f\"\\nüìä Field-Level Result Table:\")\n",
    "print(f\"{'Field':<15} {'GT Present':<12} {'MR Present':<12} {'Matches':<10} {'Total Comp':<12} {'Accuracy':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for field, stats in accuracy_df.iterrows():\n",
    "    print(f\"{field:<15} {stats['ground_truth_present']:<12} {stats['model_response_present']:<12} \"\n",
    "          f\"{stats['correct']:<10} {stats['total_comparisons']:<12} {stats['accuracy']:<10.2f}%\")\n",
    "\n",
    "# Overall statistics\n",
    "total_records = len(evaluation_data)\n",
    "avg_accuracy = accuracy_df['accuracy'].mean()\n",
    "\n",
    "print(f\"\\nüìà Summary Statistics:\")\n",
    "print(f\"Total records with model_response: {total_records}\")\n",
    "print(f\"Average field accuracy: {avg_accuracy:.2f}%\")\n",
    "print(f\"Fields evaluated: {len(field_accuracy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis by field\n",
    "def analyze_field_performance(evaluation_data: List[Dict[str, Any]], field_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze performance for a specific field\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    mismatches = []\n",
    "    missing_in_gt = []\n",
    "    missing_in_mr = []\n",
    "    \n",
    "    for i, record in enumerate(evaluation_data):\n",
    "        gt = record['ground_truth']\n",
    "        mr = record['model_response']\n",
    "        \n",
    "        if field_name in gt and field_name in mr:\n",
    "            gt_val = str(gt[field_name]).strip().lower() if gt[field_name] is not None else ''\n",
    "            mr_val = str(mr[field_name]).strip().lower() if mr[field_name] is not None else ''\n",
    "            \n",
    "            if gt_val == mr_val:\n",
    "                matches.append({\n",
    "                    'index': i,\n",
    "                    'ground_truth': gt[field_name],\n",
    "                    'model_response': mr[field_name]\n",
    "                })\n",
    "            else:\n",
    "                mismatches.append({\n",
    "                    'index': i,\n",
    "                    'ground_truth': gt[field_name],\n",
    "                    'model_response': mr[field_name]\n",
    "                })\n",
    "        elif field_name in gt and field_name not in mr:\n",
    "            missing_in_mr.append({\n",
    "                'index': i,\n",
    "                'ground_truth': gt[field_name]\n",
    "            })\n",
    "        elif field_name not in gt and field_name in mr:\n",
    "            missing_in_gt.append({\n",
    "                'index': i,\n",
    "                'model_response': mr[field_name]\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        'matches': matches,\n",
    "        'mismatches': mismatches,\n",
    "        'missing_in_ground_truth': missing_in_gt,\n",
    "        'missing_in_model_response': missing_in_mr\n",
    "    }\n",
    "\n",
    "# Analyze specific fields (you can change these field names)\n",
    "fields_to_analyze = ['invoice_date', 'invoice_no', 'amount', 'buyer_gstin', 'seller_gstin']\n",
    "\n",
    "for field in fields_to_analyze:\n",
    "    if field in field_accuracy:\n",
    "        print(f\"\\nüîç Detailed Analysis for '{field}':\")\n",
    "        analysis = analyze_field_performance(evaluation_data, field)\n",
    "        \n",
    "        print(f\"  ‚úÖ Matches: {len(analysis['matches'])}\")\n",
    "        print(f\"  ‚ùå Mismatches: {len(analysis['mismatches'])}\")\n",
    "        print(f\"  üîç Missing in Model Response: {len(analysis['missing_in_model_response'])}\")\n",
    "        print(f\"  üîç Missing in Ground Truth: {len(analysis['missing_in_ground_truth'])}\")\n",
    "        \n",
    "        # Show first few mismatches as examples\n",
    "        if analysis['mismatches']:\n",
    "            print(f\"  üìù Sample Mismatches (first 3):\")\n",
    "            for i, mismatch in enumerate(analysis['mismatches'][:3]):\n",
    "                print(f\"    {i+1}. GT: '{mismatch['ground_truth']}' vs MR: '{mismatch['model_response']}'\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Field '{field}' not found in the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON file\n",
    "results = {\n",
    "    'summary': {\n",
    "        'total_records_in_file': len(finvix_data),\n",
    "        'records_with_model_response': len(filtered_data),\n",
    "        'records_without_model_response': len(finvix_data) - len(filtered_data),\n",
    "        'average_field_accuracy': float(avg_accuracy),\n",
    "        'fields_evaluated': len(field_accuracy)\n",
    "    },\n",
    "    'field_accuracy': {field: {\n",
    "        'accuracy': float(stats['accuracy']),\n",
    "        'correct': int(stats['correct']),\n",
    "        'total_comparisons': int(stats['total_comparisons']),\n",
    "        'ground_truth_present': int(stats['ground_truth_present']),\n",
    "        'model_response_present': int(stats['model_response_present'])\n",
    "    } for field, stats in field_accuracy.items()}\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "output_file = 'finvix_7000_evaluation_results.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nüíæ Results saved to: {output_file}\")\n",
    "print(f\"\\n‚úÖ Evaluation completed successfully!\")\n",
    "print(f\"   - Only processed records with 'model_response' key\")\n",
    "print(f\"   - {len(filtered_data)} out of {len(finvix_data)} records were evaluated\")\n",
    "print(f\"   - Average accuracy across all fields: {avg_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
