{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Finvix 7000 Model Response Evaluation\n",
      "============================================================\n",
      "📂 Loading finvix_7000.json...\n",
      "Total records loaded: 7000\n",
      "\n",
      "🔍 Filtering records with model_response...\n",
      "Records with model_response: 6948\n",
      "Records without model_response: 52\n",
      "\n",
      "⚙️  Processing data for evaluation...\n",
      "Prepared 6948 records for evaluation\n",
      "\n",
      "📊 Calculating field-level accuracy...\n",
      "\n",
      "================================================================================\n",
      "FIELD-LEVEL ACCURACY RESULTS\n",
      "================================================================================\n",
      "\n",
      "📊 Field-Level Result Table:\n",
      "Field           Ground Truth Model Response Matches    Total Comp   Accuracy  \n",
      "--------------------------------------------------------------------------------\n",
      "invoice_date    6948.0       6913.0       6018.0     6913.0       87.05     %\n",
      "amount          6948.0       6913.0       5698.0     6913.0       82.42     %\n",
      "invoice_no      6948.0       6913.0       4460.0     6913.0       64.52     %\n",
      "buyer_gstin     6948.0       6913.0       4089.0     6913.0       59.15     %\n",
      "seller_gstin    6948.0       6905.0       3324.0     6905.0       48.14     %\n",
      "error           0.0          35.0         0.0        0.0          0.00      %\n",
      "raw_response    0.0          35.0         0.0        0.0          0.00      %\n",
      "seller_gtinr    0.0          1.0          0.0        0.0          0.00      %\n",
      "seller_gtinz    0.0          2.0          0.0        0.0          0.00      %\n",
      "seller_gtin     0.0          2.0          0.0        0.0          0.00      %\n",
      "seller_gtin n   0.0          2.0          0.0        0.0          0.00      %\n",
      "\n",
      "📈 Summary Statistics:\n",
      "Total records with model_response: 6948\n",
      "Average field accuracy: 31.03%\n",
      "Fields evaluated: 11\n",
      "\n",
      "🔍 Detailed Field Analysis:\n",
      "--------------------------------------------------\n",
      "\n",
      "🔍 Detailed Analysis for 'invoice_date':\n",
      "  ✅ Matches: 6018\n",
      "  ❌ Mismatches: 895\n",
      "  🔍 Missing in Model Response: 35\n",
      "  🔍 Missing in Ground Truth: 0\n",
      "  📝 Sample Mismatches (first 3):\n",
      "    1. GT: '02-05-2025' vs MR: '17-05-2024'\n",
      "    2. GT: '21-04-2025' vs MR: '12-04-25'\n",
      "    3. GT: '18-06-2025' vs MR: '16-06-2025'\n",
      "\n",
      "🔍 Detailed Analysis for 'invoice_no':\n",
      "  ✅ Matches: 4460\n",
      "  ❌ Mismatches: 2453\n",
      "  🔍 Missing in Model Response: 35\n",
      "  🔍 Missing in Ground Truth: 0\n",
      "  📝 Sample Mismatches (first 3):\n",
      "    1. GT: 'SP-52' vs MR: '175D'\n",
      "    2. GT: '0053' vs MR: 'PO/25/HA/651'\n",
      "    3. GT: '25-26/JECTI-115' vs MR: '3393333323006351'\n",
      "\n",
      "🔍 Detailed Analysis for 'amount':\n",
      "  ✅ Matches: 5698\n",
      "  ❌ Mismatches: 1215\n",
      "  🔍 Missing in Model Response: 35\n",
      "  🔍 Missing in Ground Truth: 0\n",
      "  📝 Sample Mismatches (first 3):\n",
      "    1. GT: '237026' vs MR: '23'\n",
      "    2. GT: '1192870' vs MR: '949050'\n",
      "    3. GT: '76056' vs MR: '76560'\n",
      "\n",
      "🔍 Detailed Analysis for 'buyer_gstin':\n",
      "  ✅ Matches: 4089\n",
      "  ❌ Mismatches: 2824\n",
      "  🔍 Missing in Model Response: 35\n",
      "  🔍 Missing in Ground Truth: 0\n",
      "  📝 Sample Mismatches (first 3):\n",
      "    1. GT: '33BPEPS0141C2ZH' vs MR: '33AFIPJ9961Q1ZU'\n",
      "    2. GT: '05AAJCB4606R1ZB' vs MR: '05AAECPG7376H1ZO'\n",
      "    3. GT: '09CHZPM7467B1ZL' vs MR: '39BQPPG4000D1ZN'\n",
      "\n",
      "🔍 Detailed Analysis for 'seller_gstin':\n",
      "  ✅ Matches: 3324\n",
      "  ❌ Mismatches: 3581\n",
      "  🔍 Missing in Model Response: 43\n",
      "  🔍 Missing in Ground Truth: 0\n",
      "  📝 Sample Mismatches (first 3):\n",
      "    1. GT: '33ALUPK0136E1ZC' vs MR: '33ALUPR9961Q1ZA'\n",
      "    2. GT: '99AAJCB4606RSPT' vs MR: '05AAJCB4606R1ZB'\n",
      "    3. GT: '09AAMFJ9369F1ZP' vs MR: '39BQPPG4000D1ZN'\n",
      "\n",
      "💾 Saving results...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Finvix 500 Test - Model Response Evaluation\n",
    "\n",
    "This script processes the finvix_7000.json file and only performs evaluation/matching \n",
    "when the `model_response` key is available.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List, Optional\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "import re\n",
    "\n",
    "def normalize_date(date_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a date string into 'YYYY-MM-DD' format if possible.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed = parser.parse(date_str, dayfirst=True)\n",
    "        return parsed.strftime('%Y-%m-%d')\n",
    "    except (ValueError, TypeError, parser.ParserError):\n",
    "        return str(date_str).strip().lower()  # Fallback to lowercase string\n",
    "\n",
    "def normalize_amount(amount_str: Any) -> str:\n",
    "    \"\"\"\n",
    "    Normalize amount string to a standard float string with 2 decimal places.\n",
    "    Removes commas, currency symbols, and whitespace.\n",
    "    \"\"\"\n",
    "    if isinstance(amount_str, (int, float)):\n",
    "        return f\"{float(amount_str):.2f}\"\n",
    "    \n",
    "    if not isinstance(amount_str, str):\n",
    "        return ''\n",
    "\n",
    "    # Remove currency symbols, commas, and extra characters\n",
    "    cleaned = re.sub(r'[₹$,]', '', amount_str)\n",
    "    cleaned = cleaned.strip()\n",
    "    \n",
    "    try:\n",
    "        value = float(cleaned)\n",
    "        return f\"{value:.2f}\"\n",
    "    except ValueError:\n",
    "        return cleaned.lower()\n",
    "\n",
    "\n",
    "def load_finvix_data(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load the finvix JSON data from file\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def filter_records_with_model_response(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Filter records that contain the 'model_response' key\n",
    "    \"\"\"\n",
    "    filtered_data = []\n",
    "    for record in data:\n",
    "        if 'model_response' in record and record['model_response'] is not None:\n",
    "            filtered_data.append(record)\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def parse_ground_truth(ground_truth_str: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse the ground truth JSON string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return json.loads(ground_truth_str)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def calculate_field_accuracy(evaluation_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Calculate field-level accuracy between ground truth and model response\n",
    "    \"\"\"\n",
    "    field_stats = {}\n",
    "    \n",
    "    for record in evaluation_data:\n",
    "        ground_truth = record['ground_truth']\n",
    "        model_response = record['model_response']\n",
    "        \n",
    "        # Get all unique fields from both ground truth and model response\n",
    "        all_fields = set(ground_truth.keys()) | set(model_response.keys())\n",
    "        \n",
    "        for field in all_fields:\n",
    "            if field not in field_stats:\n",
    "                field_stats[field] = {\n",
    "                    'total': 0,\n",
    "                    'correct': 0,\n",
    "                    'ground_truth_present': 0,\n",
    "                    'model_response_present': 0\n",
    "                }\n",
    "            \n",
    "            field_stats[field]['total'] += 1\n",
    "            \n",
    "            # Check if field is present in ground truth\n",
    "            if field in ground_truth:\n",
    "                field_stats[field]['ground_truth_present'] += 1\n",
    "            \n",
    "            # Check if field is present in model response\n",
    "            if field in model_response:\n",
    "                field_stats[field]['model_response_present'] += 1\n",
    "            \n",
    "            # Check if values match (only if both are present)\n",
    "            if field in ground_truth and field in model_response:\n",
    "                gt_raw = ground_truth.get(field, '')\n",
    "                mr_raw = model_response.get(field, '')\n",
    "                field_lower = field.lower()\n",
    "                if 'date' in field_lower:\n",
    "                    gt_value = normalize_date(gt_raw)\n",
    "                    mr_value = normalize_date(mr_raw)\n",
    "                elif 'amount' in field_lower or 'total' in field_lower:\n",
    "                    gt_value = normalize_amount(gt_raw)\n",
    "                    mr_value = normalize_amount(mr_raw)\n",
    "                else:\n",
    "                    gt_value = str(gt_raw).strip().lower() if gt_raw is not None else ''\n",
    "                    mr_value = str(mr_raw).strip().lower() if mr_raw is not None else ''\n",
    "\n",
    "                # gt_raw = ground_truth.get(field, '')\n",
    "                # mr_raw = model_response.get(field, '')\n",
    "                # if 'date' in field.lower():\n",
    "                #     gt_value = normalize_date(gt_raw)\n",
    "                #     mr_value = normalize_date(mr_raw)\n",
    "                # else:\n",
    "                #     gt_value = str(gt_raw).strip().lower() if gt_raw is not None else ''\n",
    "                #     mr_value = str(mr_raw).strip().lower() if mr_raw is not None else ''\n",
    "                if gt_value == mr_value:\n",
    "                    field_stats[field]['correct'] += 1\n",
    "    \n",
    "    # Calculate accuracy percentages\n",
    "    field_accuracy = {}\n",
    "    for field, stats in field_stats.items():\n",
    "        total_comparisons = min(stats['ground_truth_present'], stats['model_response_present'])\n",
    "        accuracy = (stats['correct'] / total_comparisons * 100) if total_comparisons > 0 else 0\n",
    "        \n",
    "        field_accuracy[field] = {\n",
    "            'accuracy': accuracy,\n",
    "            'correct': stats['correct'],\n",
    "            'total_comparisons': total_comparisons,\n",
    "            'ground_truth_present': stats['ground_truth_present'],\n",
    "            'model_response_present': stats['model_response_present']\n",
    "        }\n",
    "    \n",
    "    return field_accuracy\n",
    "\n",
    "\n",
    "def analyze_field_performance(evaluation_data: List[Dict[str, Any]], field_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze performance for a specific field\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    mismatches = []\n",
    "    missing_in_gt = []\n",
    "    missing_in_mr = []\n",
    "    \n",
    "    for i, record in enumerate(evaluation_data):\n",
    "        gt = record['ground_truth']\n",
    "        mr = record['model_response']\n",
    "        \n",
    "        if field_name in gt and field_name in mr:\n",
    "            gt_raw = gt.get(field_name, '')\n",
    "            mr_raw = mr.get(field_name, '')\n",
    "            field_lower = field_name.lower()\n",
    "            if 'date' in field_lower:\n",
    "                gt_val = normalize_date(gt_raw)\n",
    "                mr_val = normalize_date(mr_raw)\n",
    "            elif 'amount' in field_lower or 'total' in field_lower:\n",
    "                gt_val = normalize_amount(gt_raw)\n",
    "                mr_val = normalize_amount(mr_raw)\n",
    "            else:\n",
    "                gt_val = str(gt_raw).strip().lower() if gt_raw is not None else ''\n",
    "                mr_val = str(mr_raw).strip().lower() if mr_raw is not None else ''\n",
    "\n",
    "\n",
    "            # if 'date' in field_name.lower():\n",
    "            #     gt_val = normalize_date(gt_raw)\n",
    "            #     mr_val = normalize_date(mr_raw)\n",
    "            # else:\n",
    "            #     gt_val = str(gt_raw).strip().lower() if gt_raw is not None else ''\n",
    "            #     mr_val = str(mr_raw).strip().lower() if mr_raw is not None else ''\n",
    "\n",
    "            # gt_val = str(gt[field_name]).strip().lower() if gt[field_name] is not None else ''\n",
    "            # mr_val = str(mr[field_name]).strip().lower() if mr[field_name] is not None else ''\n",
    "            \n",
    "            if gt_val == mr_val:\n",
    "                matches.append({\n",
    "                    'index': i,\n",
    "                    'ground_truth': gt[field_name],\n",
    "                    'model_response': mr[field_name]\n",
    "                })\n",
    "            else:\n",
    "                mismatches.append({\n",
    "                    'index': i,\n",
    "                    'ground_truth': gt[field_name],\n",
    "                    'model_response': mr[field_name]\n",
    "                })\n",
    "        elif field_name in gt and field_name not in mr:\n",
    "            missing_in_mr.append({\n",
    "                'index': i,\n",
    "                'ground_truth': gt[field_name]\n",
    "            })\n",
    "        elif field_name not in gt and field_name in mr:\n",
    "            missing_in_gt.append({\n",
    "                'index': i,\n",
    "                'model_response': mr[field_name]\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        'matches': matches,\n",
    "        'mismatches': mismatches,\n",
    "        'missing_in_ground_truth': missing_in_gt,\n",
    "        'missing_in_model_response': missing_in_mr\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the evaluation\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting Finvix 7000 Model Response Evaluation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"📂 Loading finvix_7000.json...\")\n",
    "    finvix_data = load_finvix_data('finvix_7000_with_static_instructions_cleaned_data.json')\n",
    "    print(f\"Total records loaded: {len(finvix_data)}\")\n",
    "    \n",
    "    # Filter records with model_response\n",
    "    print(\"\\n🔍 Filtering records with model_response...\")\n",
    "    filtered_data = filter_records_with_model_response(finvix_data)\n",
    "    print(f\"Records with model_response: {len(filtered_data)}\")\n",
    "    print(f\"Records without model_response: {len(finvix_data) - len(filtered_data)}\")\n",
    "    \n",
    "    if len(filtered_data) == 0:\n",
    "        print(\"❌ No records found with model_response key. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Process filtered data for evaluation\n",
    "    print(\"\\n⚙️  Processing data for evaluation...\")\n",
    "    evaluation_data = []\n",
    "    for record in filtered_data:\n",
    "        ground_truth = parse_ground_truth(record.get('final_ground_truth', '{}'))\n",
    "        model_response = record.get('model_response', {})\n",
    "        \n",
    "        evaluation_record = {\n",
    "            'ground_truth': ground_truth,\n",
    "            'model_response': model_response,\n",
    "            'instructions': record.get('final_instructions', ''),\n",
    "            'input': record.get('final_input', '')\n",
    "        }\n",
    "        evaluation_data.append(evaluation_record)\n",
    "    \n",
    "    print(f\"Prepared {len(evaluation_data)} records for evaluation\")\n",
    "    \n",
    "    # Calculate field accuracy\n",
    "    print(\"\\n📊 Calculating field-level accuracy...\")\n",
    "    field_accuracy = calculate_field_accuracy(evaluation_data)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FIELD-LEVEL ACCURACY RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create DataFrame for better visualization\n",
    "    accuracy_df = pd.DataFrame.from_dict(field_accuracy, orient='index')\n",
    "    accuracy_df = accuracy_df.sort_values('accuracy', ascending=False)\n",
    "\n",
    "    # print(\"accuracey_df\", accuracy_df)\n",
    "    \n",
    "    print(f\"\\n📊 Field-Level Result Table:\")\n",
    "    print(f\"{'Field':<15} {'Ground Truth':<12} {'Model Response':<12} {'Matches':<10} {'Total Comp':<12} {'Accuracy':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for field, stats in accuracy_df.iterrows():\n",
    "        print(f\"{field:<15} {stats['ground_truth_present']:<12} {stats['model_response_present']:<12} \"\n",
    "              f\"{stats['correct']:<10} {stats['total_comparisons']:<12} {stats['accuracy']:<10.2f}%\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_records = len(evaluation_data)\n",
    "    avg_accuracy = accuracy_df['accuracy'].mean()\n",
    "    \n",
    "    print(f\"\\n📈 Summary Statistics:\")\n",
    "    print(f\"Total records with model_response: {total_records}\")\n",
    "    print(f\"Average field accuracy: {avg_accuracy:.2f}%\")\n",
    "    print(f\"Fields evaluated: {len(field_accuracy)}\")\n",
    "    \n",
    "    # Detailed analysis for specific fields\n",
    "    fields_to_analyze = ['invoice_date', 'invoice_no', 'amount', 'buyer_gstin', 'seller_gstin']\n",
    "    \n",
    "    print(f\"\\n🔍 Detailed Field Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for field in fields_to_analyze:\n",
    "        if field in field_accuracy:\n",
    "            print(f\"\\n🔍 Detailed Analysis for '{field}':\")\n",
    "            analysis = analyze_field_performance(evaluation_data, field)\n",
    "            \n",
    "            print(f\"  ✅ Matches: {len(analysis['matches'])}\")\n",
    "            print(f\"  ❌ Mismatches: {len(analysis['mismatches'])}\")\n",
    "            print(f\"  🔍 Missing in Model Response: {len(analysis['missing_in_model_response'])}\")\n",
    "            print(f\"  🔍 Missing in Ground Truth: {len(analysis['missing_in_ground_truth'])}\")\n",
    "            \n",
    "            # Show first few mismatches as examples\n",
    "            if analysis['mismatches']:\n",
    "                print(f\"  📝 Sample Mismatches (first 3):\")\n",
    "                for i, mismatch in enumerate(analysis['mismatches'][:3]):\n",
    "                    print(f\"    {i+1}. GT: '{mismatch['ground_truth']}' vs MR: '{mismatch['model_response']}'\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️  Field '{field}' not found in the data\")\n",
    "    \n",
    "    # Save results to JSON file\n",
    "    print(f\"\\n💾 Saving results...\")\n",
    "    results = {\n",
    "        'summary': {\n",
    "            'total_records_in_file': len(finvix_data),\n",
    "            'records_with_model_response': len(filtered_data),\n",
    "            'records_without_model_response': len(finvix_data) - len(filtered_data),\n",
    "            'average_field_accuracy': float(avg_accuracy),\n",
    "            'fields_evaluated': len(field_accuracy)\n",
    "        },\n",
    "        'field_accuracy': {field: {\n",
    "            'accuracy': float(stats['accuracy']),\n",
    "            'correct': int(stats['correct']),\n",
    "            'total_comparisons': int(stats['total_comparisons']),\n",
    "            'ground_truth_present': int(stats['ground_truth_present']),\n",
    "            'model_response_present': int(stats['model_response_present'])\n",
    "        } for field, stats in field_accuracy.items()}\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    output_file = 'finvix_500_with_static_instructions_evaluation_result.json'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # print(f\"💾 Results saved to: {output_file}\")\n",
    "    # print(f\"\\n✅ Evaluation completed successfully!\")\n",
    "    # print(f\"   - Only processed records with 'model_response' key\")\n",
    "    # print(f\"   - {len(filtered_data)} out of {len(finvix_data)} records were evaluated\")\n",
    "    # print(f\"   - Average accuracy across all fields: {avg_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
