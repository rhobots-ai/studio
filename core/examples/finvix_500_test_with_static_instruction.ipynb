{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Finvix 7000 Model Response Evaluation\n",
      "============================================================\n",
      "üìÇ Loading finvix_7000.json...\n",
      "Total records loaded: 7000\n",
      "\n",
      "üîç Filtering records with model_response...\n",
      "Records with model_response: 6948\n",
      "Records without model_response: 52\n",
      "\n",
      "‚öôÔ∏è  Processing data for evaluation...\n",
      "Prepared 6948 records for evaluation\n",
      "\n",
      "üìä Calculating field-level accuracy...\n",
      "\n",
      "================================================================================\n",
      "FIELD-LEVEL ACCURACY RESULTS\n",
      "================================================================================\n",
      "\n",
      "üìä Field-Level Result Table:\n",
      "Field           Ground Truth Model Response Matches    Total Comp   Accuracy  \n",
      "--------------------------------------------------------------------------------\n",
      "invoice_date    6948.0       6913.0       6018.0     6913.0       87.05     %\n",
      "amount          6948.0       6913.0       5698.0     6913.0       82.42     %\n",
      "invoice_no      6948.0       6913.0       4460.0     6913.0       64.52     %\n",
      "buyer_gstin     6948.0       6913.0       4089.0     6913.0       59.15     %\n",
      "seller_gstin    6948.0       6905.0       3324.0     6905.0       48.14     %\n",
      "error           0.0          35.0         0.0        0.0          0.00      %\n",
      "raw_response    0.0          35.0         0.0        0.0          0.00      %\n",
      "seller_gtinr    0.0          1.0          0.0        0.0          0.00      %\n",
      "seller_gtinz    0.0          2.0          0.0        0.0          0.00      %\n",
      "seller_gtin     0.0          2.0          0.0        0.0          0.00      %\n",
      "seller_gtin n   0.0          2.0          0.0        0.0          0.00      %\n",
      "\n",
      "üìà Summary Statistics:\n",
      "Total records with model_response: 6948\n",
      "Average field accuracy: 31.03%\n",
      "Fields evaluated: 11\n",
      "\n",
      "üîç Detailed Field Analysis:\n",
      "--------------------------------------------------\n",
      "\n",
      "üîç Detailed Analysis for 'invoice_date':\n",
      "  ‚úÖ Matches: 6018\n",
      "  ‚ùå Mismatches: 895\n",
      "  üîç Missing in Model Response: 35\n",
      "  üîç Missing in Ground Truth: 0\n",
      "  üìù Sample Mismatches (first 3):\n",
      "    1. GT: '02-05-2025' vs MR: '17-05-2024'\n",
      "    2. GT: '21-04-2025' vs MR: '12-04-25'\n",
      "    3. GT: '18-06-2025' vs MR: '16-06-2025'\n",
      "\n",
      "üîç Detailed Analysis for 'invoice_no':\n",
      "  ‚úÖ Matches: 4460\n",
      "  ‚ùå Mismatches: 2453\n",
      "  üîç Missing in Model Response: 35\n",
      "  üîç Missing in Ground Truth: 0\n",
      "  üìù Sample Mismatches (first 3):\n",
      "    1. GT: 'SP-52' vs MR: '175D'\n",
      "    2. GT: '0053' vs MR: 'PO/25/HA/651'\n",
      "    3. GT: '25-26/JECTI-115' vs MR: '3393333323006351'\n",
      "\n",
      "üîç Detailed Analysis for 'amount':\n",
      "  ‚úÖ Matches: 5698\n",
      "  ‚ùå Mismatches: 1215\n",
      "  üîç Missing in Model Response: 35\n",
      "  üîç Missing in Ground Truth: 0\n",
      "  üìù Sample Mismatches (first 3):\n",
      "    1. GT: '237026' vs MR: '23'\n",
      "    2. GT: '1192870' vs MR: '949050'\n",
      "    3. GT: '76056' vs MR: '76560'\n",
      "\n",
      "üîç Detailed Analysis for 'buyer_gstin':\n",
      "  ‚úÖ Matches: 4089\n",
      "  ‚ùå Mismatches: 2824\n",
      "  üîç Missing in Model Response: 35\n",
      "  üîç Missing in Ground Truth: 0\n",
      "  üìù Sample Mismatches (first 3):\n",
      "    1. GT: '33BPEPS0141C2ZH' vs MR: '33AFIPJ9961Q1ZU'\n",
      "    2. GT: '05AAJCB4606R1ZB' vs MR: '05AAECPG7376H1ZO'\n",
      "    3. GT: '09CHZPM7467B1ZL' vs MR: '39BQPPG4000D1ZN'\n",
      "\n",
      "üîç Detailed Analysis for 'seller_gstin':\n",
      "  ‚úÖ Matches: 3324\n",
      "  ‚ùå Mismatches: 3581\n",
      "  üîç Missing in Model Response: 43\n",
      "  üîç Missing in Ground Truth: 0\n",
      "  üìù Sample Mismatches (first 3):\n",
      "    1. GT: '33ALUPK0136E1ZC' vs MR: '33ALUPR9961Q1ZA'\n",
      "    2. GT: '99AAJCB4606RSPT' vs MR: '05AAJCB4606R1ZB'\n",
      "    3. GT: '09AAMFJ9369F1ZP' vs MR: '39BQPPG4000D1ZN'\n",
      "\n",
      "üíæ Saving results...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Finvix 500 Test - Model Response Evaluation\n",
    "\n",
    "This script processes the finvix_7000.json file and only performs evaluation/matching \n",
    "when the `model_response` key is available.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List, Optional\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "import re\n",
    "\n",
    "def normalize_date(date_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a date string into 'YYYY-MM-DD' format if possible.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed = parser.parse(date_str, dayfirst=True)\n",
    "        return parsed.strftime('%Y-%m-%d')\n",
    "    except (ValueError, TypeError, parser.ParserError):\n",
    "        return str(date_str).strip().lower()  # Fallback to lowercase string\n",
    "\n",
    "def normalize_amount(amount_str: Any) -> str:\n",
    "    \"\"\"\n",
    "    Normalize amount string to a standard float string with 2 decimal places.\n",
    "    Removes commas, currency symbols, and whitespace.\n",
    "    \"\"\"\n",
    "    if isinstance(amount_str, (int, float)):\n",
    "        return f\"{float(amount_str):.2f}\"\n",
    "    \n",
    "    if not isinstance(amount_str, str):\n",
    "        return ''\n",
    "\n",
    "    # Remove currency symbols, commas, and extra characters\n",
    "    cleaned = re.sub(r'[‚Çπ$,]', '', amount_str)\n",
    "    cleaned = cleaned.strip()\n",
    "    \n",
    "    try:\n",
    "        value = float(cleaned)\n",
    "        return f\"{value:.2f}\"\n",
    "    except ValueError:\n",
    "        return cleaned.lower()\n",
    "\n",
    "\n",
    "def load_finvix_data(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load the finvix JSON data from file\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def filter_records_with_model_response(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Filter records that contain the 'model_response' key\n",
    "    \"\"\"\n",
    "    filtered_data = []\n",
    "    for record in data:\n",
    "        if 'model_response' in record and record['model_response'] is not None:\n",
    "            filtered_data.append(record)\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def parse_ground_truth(ground_truth_str: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse the ground truth JSON string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return json.loads(ground_truth_str)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def calculate_field_accuracy(evaluation_data: List[Dict[str, Any]]) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Calculate field-level accuracy between ground truth and model response\n",
    "    \"\"\"\n",
    "    field_stats = {}\n",
    "    \n",
    "    for record in evaluation_data:\n",
    "        ground_truth = record['ground_truth']\n",
    "        model_response = record['model_response']\n",
    "        \n",
    "        # Get all unique fields from both ground truth and model response\n",
    "        all_fields = set(ground_truth.keys()) | set(model_response.keys())\n",
    "        \n",
    "        for field in all_fields:\n",
    "            if field not in field_stats:\n",
    "                field_stats[field] = {\n",
    "                    'total': 0,\n",
    "                    'correct': 0,\n",
    "                    'ground_truth_present': 0,\n",
    "                    'model_response_present': 0\n",
    "                }\n",
    "            \n",
    "            field_stats[field]['total'] += 1\n",
    "            \n",
    "            # Check if field is present in ground truth\n",
    "            if field in ground_truth:\n",
    "                field_stats[field]['ground_truth_present'] += 1\n",
    "            \n",
    "            # Check if field is present in model response\n",
    "            if field in model_response:\n",
    "                field_stats[field]['model_response_present'] += 1\n",
    "            \n",
    "            # Check if values match (only if both are present)\n",
    "            if field in ground_truth and field in model_response:\n",
    "                gt_raw = ground_truth.get(field, '')\n",
    "                mr_raw = model_response.get(field, '')\n",
    "                field_lower = field.lower()\n",
    "                if 'date' in field_lower:\n",
    "                    gt_value = normalize_date(gt_raw)\n",
    "                    mr_value = normalize_date(mr_raw)\n",
    "                elif 'amount' in field_lower or 'total' in field_lower:\n",
    "                    gt_value = normalize_amount(gt_raw)\n",
    "                    mr_value = normalize_amount(mr_raw)\n",
    "                else:\n",
    "                    gt_value = str(gt_raw).strip().lower() if gt_raw is not None else ''\n",
    "                    mr_value = str(mr_raw).strip().lower() if mr_raw is not None else ''\n",
    "\n",
    "                # gt_raw = ground_truth.get(field, '')\n",
    "                # mr_raw = model_response.get(field, '')\n",
    "                # if 'date' in field.lower():\n",
    "                #     gt_value = normalize_date(gt_raw)\n",
    "                #     mr_value = normalize_date(mr_raw)\n",
    "                # else:\n",
    "                #     gt_value = str(gt_raw).strip().lower() if gt_raw is not None else ''\n",
    "                #     mr_value = str(mr_raw).strip().lower() if mr_raw is not None else ''\n",
    "                if gt_value == mr_value:\n",
    "                    field_stats[field]['correct'] += 1\n",
    "    \n",
    "    # Calculate accuracy percentages\n",
    "    field_accuracy = {}\n",
    "    for field, stats in field_stats.items():\n",
    "        total_comparisons = min(stats['ground_truth_present'], stats['model_response_present'])\n",
    "        accuracy = (stats['correct'] / total_comparisons * 100) if total_comparisons > 0 else 0\n",
    "        \n",
    "        field_accuracy[field] = {\n",
    "            'accuracy': accuracy,\n",
    "            'correct': stats['correct'],\n",
    "            'total_comparisons': total_comparisons,\n",
    "            'ground_truth_present': stats['ground_truth_present'],\n",
    "            'model_response_present': stats['model_response_present']\n",
    "        }\n",
    "    \n",
    "    return field_accuracy\n",
    "\n",
    "\n",
    "def analyze_field_performance(evaluation_data: List[Dict[str, Any]], field_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze performance for a specific field\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    mismatches = []\n",
    "    missing_in_gt = []\n",
    "    missing_in_mr = []\n",
    "    \n",
    "    for i, record in enumerate(evaluation_data):\n",
    "        gt = record['ground_truth']\n",
    "        mr = record['model_response']\n",
    "        \n",
    "        if field_name in gt and field_name in mr:\n",
    "            gt_raw = gt.get(field_name, '')\n",
    "            mr_raw = mr.get(field_name, '')\n",
    "            field_lower = field_name.lower()\n",
    "            if 'date' in field_lower:\n",
    "                gt_val = normalize_date(gt_raw)\n",
    "                mr_val = normalize_date(mr_raw)\n",
    "            elif 'amount' in field_lower or 'total' in field_lower:\n",
    "                gt_val = normalize_amount(gt_raw)\n",
    "                mr_val = normalize_amount(mr_raw)\n",
    "            else:\n",
    "                gt_val = str(gt_raw).strip().lower() if gt_raw is not None else ''\n",
    "                mr_val = str(mr_raw).strip().lower() if mr_raw is not None else ''\n",
    "\n",
    "\n",
    "            # if 'date' in field_name.lower():\n",
    "            #     gt_val = normalize_date(gt_raw)\n",
    "            #     mr_val = normalize_date(mr_raw)\n",
    "            # else:\n",
    "            #     gt_val = str(gt_raw).strip().lower() if gt_raw is not None else ''\n",
    "            #     mr_val = str(mr_raw).strip().lower() if mr_raw is not None else ''\n",
    "\n",
    "            # gt_val = str(gt[field_name]).strip().lower() if gt[field_name] is not None else ''\n",
    "            # mr_val = str(mr[field_name]).strip().lower() if mr[field_name] is not None else ''\n",
    "            \n",
    "            if gt_val == mr_val:\n",
    "                matches.append({\n",
    "                    'index': i,\n",
    "                    'ground_truth': gt[field_name],\n",
    "                    'model_response': mr[field_name]\n",
    "                })\n",
    "            else:\n",
    "                mismatches.append({\n",
    "                    'index': i,\n",
    "                    'ground_truth': gt[field_name],\n",
    "                    'model_response': mr[field_name]\n",
    "                })\n",
    "        elif field_name in gt and field_name not in mr:\n",
    "            missing_in_mr.append({\n",
    "                'index': i,\n",
    "                'ground_truth': gt[field_name]\n",
    "            })\n",
    "        elif field_name not in gt and field_name in mr:\n",
    "            missing_in_gt.append({\n",
    "                'index': i,\n",
    "                'model_response': mr[field_name]\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        'matches': matches,\n",
    "        'mismatches': mismatches,\n",
    "        'missing_in_ground_truth': missing_in_gt,\n",
    "        'missing_in_model_response': missing_in_mr\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the evaluation\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting Finvix 7000 Model Response Evaluation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"üìÇ Loading finvix_7000.json...\")\n",
    "    finvix_data = load_finvix_data('finvix_7000_with_static_instructions_cleaned_data.json')\n",
    "    print(f\"Total records loaded: {len(finvix_data)}\")\n",
    "    \n",
    "    # Filter records with model_response\n",
    "    print(\"\\nüîç Filtering records with model_response...\")\n",
    "    filtered_data = filter_records_with_model_response(finvix_data)\n",
    "    print(f\"Records with model_response: {len(filtered_data)}\")\n",
    "    print(f\"Records without model_response: {len(finvix_data) - len(filtered_data)}\")\n",
    "    \n",
    "    if len(filtered_data) == 0:\n",
    "        print(\"‚ùå No records found with model_response key. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Process filtered data for evaluation\n",
    "    print(\"\\n‚öôÔ∏è  Processing data for evaluation...\")\n",
    "    evaluation_data = []\n",
    "    for record in filtered_data:\n",
    "        ground_truth = parse_ground_truth(record.get('final_ground_truth', '{}'))\n",
    "        model_response = record.get('model_response', {})\n",
    "        \n",
    "        evaluation_record = {\n",
    "            'ground_truth': ground_truth,\n",
    "            'model_response': model_response,\n",
    "            'instructions': record.get('final_instructions', ''),\n",
    "            'input': record.get('final_input', '')\n",
    "        }\n",
    "        evaluation_data.append(evaluation_record)\n",
    "    \n",
    "    print(f\"Prepared {len(evaluation_data)} records for evaluation\")\n",
    "    \n",
    "    # Calculate field accuracy\n",
    "    print(\"\\nüìä Calculating field-level accuracy...\")\n",
    "    field_accuracy = calculate_field_accuracy(evaluation_data)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FIELD-LEVEL ACCURACY RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create DataFrame for better visualization\n",
    "    accuracy_df = pd.DataFrame.from_dict(field_accuracy, orient='index')\n",
    "    accuracy_df = accuracy_df.sort_values('accuracy', ascending=False)\n",
    "\n",
    "    # print(\"accuracey_df\", accuracy_df)\n",
    "    \n",
    "    print(f\"\\nüìä Field-Level Result Table:\")\n",
    "    print(f\"{'Field':<15} {'Ground Truth':<12} {'Model Response':<12} {'Matches':<10} {'Total Comp':<12} {'Accuracy':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for field, stats in accuracy_df.iterrows():\n",
    "        print(f\"{field:<15} {stats['ground_truth_present']:<12} {stats['model_response_present']:<12} \"\n",
    "              f\"{stats['correct']:<10} {stats['total_comparisons']:<12} {stats['accuracy']:<10.2f}%\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_records = len(evaluation_data)\n",
    "    avg_accuracy = accuracy_df['accuracy'].mean()\n",
    "    \n",
    "    print(f\"\\nüìà Summary Statistics:\")\n",
    "    print(f\"Total records with model_response: {total_records}\")\n",
    "    print(f\"Average field accuracy: {avg_accuracy:.2f}%\")\n",
    "    print(f\"Fields evaluated: {len(field_accuracy)}\")\n",
    "    \n",
    "    # Detailed analysis for specific fields\n",
    "    fields_to_analyze = ['invoice_date', 'invoice_no', 'amount', 'buyer_gstin', 'seller_gstin']\n",
    "    \n",
    "    print(f\"\\nüîç Detailed Field Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for field in fields_to_analyze:\n",
    "        if field in field_accuracy:\n",
    "            print(f\"\\nüîç Detailed Analysis for '{field}':\")\n",
    "            analysis = analyze_field_performance(evaluation_data, field)\n",
    "            \n",
    "            print(f\"  ‚úÖ Matches: {len(analysis['matches'])}\")\n",
    "            print(f\"  ‚ùå Mismatches: {len(analysis['mismatches'])}\")\n",
    "            print(f\"  üîç Missing in Model Response: {len(analysis['missing_in_model_response'])}\")\n",
    "            print(f\"  üîç Missing in Ground Truth: {len(analysis['missing_in_ground_truth'])}\")\n",
    "            \n",
    "            # Show first few mismatches as examples\n",
    "            if analysis['mismatches']:\n",
    "                print(f\"  üìù Sample Mismatches (first 3):\")\n",
    "                for i, mismatch in enumerate(analysis['mismatches'][:3]):\n",
    "                    print(f\"    {i+1}. GT: '{mismatch['ground_truth']}' vs MR: '{mismatch['model_response']}'\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  Field '{field}' not found in the data\")\n",
    "    \n",
    "    # Save results to JSON file\n",
    "    print(f\"\\nüíæ Saving results...\")\n",
    "    results = {\n",
    "        'summary': {\n",
    "            'total_records_in_file': len(finvix_data),\n",
    "            'records_with_model_response': len(filtered_data),\n",
    "            'records_without_model_response': len(finvix_data) - len(filtered_data),\n",
    "            'average_field_accuracy': float(avg_accuracy),\n",
    "            'fields_evaluated': len(field_accuracy)\n",
    "        },\n",
    "        'field_accuracy': {field: {\n",
    "            'accuracy': float(stats['accuracy']),\n",
    "            'correct': int(stats['correct']),\n",
    "            'total_comparisons': int(stats['total_comparisons']),\n",
    "            'ground_truth_present': int(stats['ground_truth_present']),\n",
    "            'model_response_present': int(stats['model_response_present'])\n",
    "        } for field, stats in field_accuracy.items()}\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    output_file = 'finvix_500_with_static_instructions_evaluation_result.json'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # print(f\"üíæ Results saved to: {output_file}\")\n",
    "    # print(f\"\\n‚úÖ Evaluation completed successfully!\")\n",
    "    # print(f\"   - Only processed records with 'model_response' key\")\n",
    "    # print(f\"   - {len(filtered_data)} out of {len(finvix_data)} records were evaluated\")\n",
    "    # print(f\"   - Average accuracy across all fields: {avg_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
